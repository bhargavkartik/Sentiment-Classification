{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5BoXrK_u3u_"
   },
   "source": [
    "# Author : Kartik B Bhargav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr9zwNA-u3vH"
   },
   "source": [
    "# Sequence Classification\n",
    "\n",
    "# Task: Aspect-level Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "FB6yxc4BM9uU",
    "outputId": "17252f48-49de-4601-8714-5fb37db6fcf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPDMoaQNNAC5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymEccUbRNEGd"
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9TH6Um2s-7d"
   },
   "source": [
    "\n",
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbWy_h2KtNgR"
   },
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuAMxdYBkv_x"
   },
   "outputs": [],
   "source": [
    "aspect_path = '/drive/My Drive/Deep Learning Course/practice-5-data/aspect_level-sentiment/aspect_level/' \n",
    "\n",
    "\n",
    "vocab = read_pickle(aspect_path, 'all_vocab.pkl')\n",
    "\n",
    "train_x = read_pickle(aspect_path, 'train_x.pkl')\n",
    "train_y = read_pickle(aspect_path, 'train_y.pkl')\n",
    "dev_x = read_pickle(aspect_path, 'dev_x.pkl')\n",
    "dev_y = read_pickle(aspect_path, 'dev_y.pkl')\n",
    "test_x = read_pickle(aspect_path, 'test_x.pkl')\n",
    "test_y = read_pickle(aspect_path, 'test_y.pkl')\n",
    "\n",
    "train_aspect = read_pickle(aspect_path, 'train_aspect.pkl')\n",
    "dev_aspect = read_pickle(aspect_path, 'dev_aspect.pkl')\n",
    "test_aspect = read_pickle(aspect_path, 'test_aspect.pkl')\n",
    "\n",
    "\n",
    "pretrain_data = read_pickle(aspect_path, 'pretrain_data.pkl')\n",
    "pretrain_label = read_pickle(aspect_path, 'pretrain_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWVgGO8RlVIJ"
   },
   "outputs": [],
   "source": [
    "class Dataiterator_doc():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, seq_length=32, decoder_dim=300, batch_size=32):      \n",
    "        self.X = X \n",
    "        self.y = y \n",
    "        self.num_data = len(X) # total number of examples\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X = self.X[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y = self.y[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        return batch_X, batch_y\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X, self.y\n",
    "class Dataiterator_aspect():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, aspect_data, seq_length=32, decoder_dim=300, batch_size=32):\n",
    "        \n",
    "        len_aspect_data = len(aspect_data[0])\n",
    "        #self.len_doc_data = len(doc_data[0])\n",
    "        \n",
    "        self.X_aspect = aspect_data[0] \n",
    "        self.y_aspect = aspect_data[1]\n",
    "        self.aspect_terms = aspect_data[2]  \n",
    "        self.num_data = len_aspect_data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        \n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X_aspect = self.X_aspect[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y_aspect = self.y_aspect[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        batch_aspect_terms = self.aspect_terms[np.array(X_ids)]\n",
    "        \n",
    "        return batch_X_aspect, batch_y_aspect, batch_aspect_terms\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X_aspect, self.y_aspect, self.aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "38IEEx0du3vW",
    "outputId": "c992e0f3-b434-4e79-9978-9fad11e0111d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Lambda, Dropout, LSTM,Bidirectional\n",
    "from keras.layers import Reshape, Activation, RepeatVector, concatenate, Concatenate, Dot, Multiply\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cduHSpsSnVue"
   },
   "outputs": [],
   "source": [
    "overal_maxlen = 82\n",
    "overal_maxlen_aspect = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhTQke3HnvHN"
   },
   "source": [
    "\n",
    "#Define Attention Network Layer\n",
    "- Define class for Attention Layer\n",
    "- Calculating the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnDX-po3_50B"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,  **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Content Attention mechanism.\n",
    "        Supports Masking.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert type(input_shape) == list\n",
    "       \n",
    "        self.steps = input_shape[0][1]\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[0][-1], input_shape[1][-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input_tensor, mask=None):\n",
    "        assert type(input_tensor) == list\n",
    "        assert type(mask) == list\n",
    "        return None\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        x = input_tensor[0]\n",
    "        aspect = input_tensor[1]\n",
    "        mask = mask[0]\n",
    "\n",
    "        ###YOUR CODE HERE###\n",
    "\n",
    "        y = K.transpose(K.dot(self.W, K.transpose(aspect)))\n",
    "        y = K.expand_dims(y, axis=-2)\n",
    "        y = K.repeat_elements(y, self.steps, axis=1)\n",
    "        eij = K.sum(x * y, axis=-1)\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        return a\n",
    "\n",
    "   \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5PSPx85EiVn"
   },
   "outputs": [],
   "source": [
    "class Average(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Average, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        if self.mask_zero:           \n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=1) / (K.sum(mask, axis=1) + K.epsilon())\n",
    "        else:\n",
    "            return K.mean(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsZtx2PbEqoh"
   },
   "source": [
    "#Establish computation Graph for model\n",
    "- Input tensors\n",
    "- Shared WordEmbedding layer \n",
    "- Attention network layer  \n",
    "- Shared BiLSTM layer\n",
    "- Shared fully connected layer(prediction layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe55OCNZEmYY"
   },
   "outputs": [],
   "source": [
    "dropout = 0.5     \n",
    "recurrent_dropout = 0.1\n",
    "vocab_size = len(vocab)\n",
    "num_outputs = 3 # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BpzACdBp3xG"
   },
   "source": [
    "##Input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "KOi3CcOxE1MG",
    "outputId": "42734d8a-572f-4497-d2bb-e0e07e9a189e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sentence_input:0\", shape=(None, 82), dtype=int32)\n",
      "Tensor(\"aspect_input:0\", shape=(None, 7), dtype=int32)\n",
      "Tensor(\"pretrain_input:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE ##### Inputs #####\n",
    "sentence_input = Input(shape=(overal_maxlen,), dtype='int32', name='sentence_input')\n",
    "print(sentence_input)\n",
    "\n",
    "aspect_input = Input(shape=(overal_maxlen_aspect,), dtype='int32', name='aspect_input')\n",
    "print(aspect_input)\n",
    "\n",
    "pretrain_input = Input(shape=(None,), dtype='int32', name='pretrain_input')\n",
    "print(pretrain_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vQ0z8KmrL3_"
   },
   "source": [
    "##Shared WordEmbedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQXlGMxU0uvk"
   },
   "outputs": [],
   "source": [
    "# word embedding layer\n",
    "word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFEhEt9EE4Sn"
   },
   "outputs": [],
   "source": [
    "# represent aspect as averaged word embedding ###\n",
    "aspect_term_embs = word_emb(aspect_input)\n",
    "aspect_embs = Average(mask_zero=True, name='aspect_emb')(aspect_term_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gGd94OGE-Gr"
   },
   "outputs": [],
   "source": [
    "## sentence representation from embedding ###\n",
    "sentence_word_emb = word_emb(sentence_input)\n",
    "pretrain_word_emb = word_emb(pretrain_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmcnAQufrc7o"
   },
   "source": [
    "##Shared BiLSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bm_yCjX_F7ml",
    "outputId": "33f161b9-5a41-47a5-bb6c-e4aa049beba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 82, 600)\n",
      "(None, None, 600)\n"
     ]
    }
   ],
   "source": [
    "## sentence representation from embedding ###\n",
    "rnn = Bidirectional(LSTM(300, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, name='bilstm'))\n",
    "\n",
    "sentence_lstm = rnn(sentence_word_emb)     # from aspect-level domain\n",
    "pretrain_lstm = rnn(pretrain_word_emb)     # from document-level domain\n",
    "\n",
    "print(sentence_lstm.shape)\n",
    "print(pretrain_lstm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99ZNrbkmrllN"
   },
   "source": [
    "##Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EbFO6avVB_9a",
    "outputId": "cc2e324b-1199-497b-9219-bf9298e6e13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 82)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attention_layer = Attention()([sentence_lstm, aspect_embs])\n",
    "print(attention_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-LY6jF8r3mO"
   },
   "source": [
    "##Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "HO5Pj6QANz7U",
    "outputId": "ebeb0f49-4ec3-4991-ded7-302909c14666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3)\n",
      "(None, 600)\n",
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dot_sentence_attention = Dot(axes=1)([sentence_lstm, attention_layer])\n",
    "dense_layer = Dense(3, activation='softmax')\n",
    "aspect_probs = dense_layer(dot_sentence_attention)\n",
    "print(aspect_probs.shape)\n",
    "\n",
    "pretrain_avg = Average(mask_zero=True)(pretrain_lstm)\n",
    "print(pretrain_avg.shape)\n",
    "\n",
    "pretrain_probs = dense_layer(pretrain_avg)\n",
    "print(pretrain_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XLv1t9Ou3vx"
   },
   "source": [
    "#Build Models for document-level and aspect-level data\n",
    "- The two models shared the embedding, BiLSTM, Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "id": "8BEjUc6JFIQL",
    "outputId": "def1b20d-bebe-47bf-d3be-e85d7e0a11b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_input (InputLayer)     (None, 82)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "aspect_input (InputLayer)       (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_emb (Embedding)            multiple             3000900     aspect_input[0][0]               \n",
      "                                                                 sentence_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) multiple             1442400     word_emb[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aspect_emb (Average)            (None, 300)          0           word_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 82)           180000      bidirectional_1[0][0]            \n",
      "                                                                 aspect_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 600)          0           bidirectional_1[0][0]            \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            1803        dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,625,103\n",
      "Trainable params: 4,625,103\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1 = Model(inputs=[pretrain_input], outputs=[pretrain_probs])\n",
    "model2 = Model(inputs=[sentence_input, aspect_input], outputs=[aspect_probs])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWqofwqBsgsn"
   },
   "source": [
    "#Train Model\n",
    "- First Train model on document-level data.\n",
    "- Then Train  model on aspect-level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSLYsZm7yPwi"
   },
   "source": [
    "##Train on document-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GHe86c6Yu3wG"
   },
   "outputs": [],
   "source": [
    "import keras.optimizers as opt\n",
    "optimizer=opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n",
    "model1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "batch_size = 128\n",
    "train_steps_epoch = len(pretrain_data)/batch_size\n",
    "batch_train_iter_doc = Dataiterator_doc(pretrain_data, pretrain_label, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "HfEvhdbhFlbR",
    "outputId": "c7f50a53-2f57-4dad-f1b3-95db778f91b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "235/234 [==============================] - 330s 1s/step - loss: 0.9422 - categorical_accuracy: 0.5457\n",
      "Epoch 2/5\n",
      "235/234 [==============================] - 328s 1s/step - loss: 0.7939 - categorical_accuracy: 0.6521\n",
      "Epoch 3/5\n",
      "235/234 [==============================] - 327s 1s/step - loss: 0.7435 - categorical_accuracy: 0.6687\n",
      "Epoch 4/5\n",
      "235/234 [==============================] - 324s 1s/step - loss: 0.7204 - categorical_accuracy: 0.6809\n",
      "Epoch 5/5\n",
      "235/234 [==============================] - 324s 1s/step - loss: 0.6604 - categorical_accuracy: 0.7207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def pretrain_generator(model, batch_train_iter_doc, pretrain_steps_epoch):\n",
    "    \n",
    "    earlystop_callbacks = [EarlyStopping(monitor='loss', patience=10),\n",
    "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
    "                                     monitor='val_loss', save_best_only=False, \\\n",
    "                                     save_weights_only=True)\n",
    "                     ]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[X, y] for X, y in batch_train_iter_doc]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "\n",
    "    history = model.fit_generator(train_gen(), \n",
    "                                  steps_per_epoch=pretrain_steps_epoch,\n",
    "                                  epochs = 5, \n",
    "                                  callbacks = earlystop_callbacks)\n",
    "\n",
    "pretrain_generator(model1, batch_train_iter_doc, train_steps_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-SYaHNMyXWX"
   },
   "source": [
    "##Train on aspect-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCLGP6HTFsKL"
   },
   "outputs": [],
   "source": [
    "train_steps_epoch = len(train_x)/batch_size\n",
    "batch_train_iter_aspect = Dataiterator_aspect([train_x, train_y, train_aspect], batch_size)\n",
    "val_steps_epoch = len(dev_x)/batch_size\n",
    "batch_val_iter_aspect = Dataiterator_aspect([dev_x, dev_y, dev_aspect], batch_size)\n",
    "\n",
    "import keras.optimizers as opt\n",
    "optimizer = opt.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clipnorm=10, clipvalue=0)\n",
    "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvT2GqG0LONz"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_generator(model, batch_train_iter, batch_val_iter, train_steps_epoch, val_steps_epoch):\n",
    "    \n",
    "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
    "                                     monitor='val_loss', save_best_only=False, \\\n",
    "                                     save_weights_only=True)\n",
    "                     ]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[[X, aspect], [y]] for X, y, aspect in batch_train_iter]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "                \n",
    "    def val_gen():\n",
    "        while True:\n",
    "            val_batches = [[[X, aspect], [y]] for X, y, aspect in batch_val_iter]\n",
    "            for val_batch in val_batches:\n",
    "                yield val_batch\n",
    "                \n",
    "    history = model.fit_generator(train_gen(), \n",
    "                                  validation_data=val_gen(),\n",
    "                                  validation_steps=val_steps_epoch, \n",
    "                                  steps_per_epoch=train_steps_epoch,\n",
    "                                  epochs = 10, \n",
    "                                  callbacks = earlystop_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "8jcndd98Mad2",
    "outputId": "d3372712-e6e7-4183-c90c-be23b3e7ee32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/14 [===============================] - 7s 475ms/step - loss: 1.0481 - categorical_accuracy: 0.5417 - val_loss: 1.2943 - val_categorical_accuracy: 0.4844\n",
      "Epoch 2/10\n",
      "15/14 [===============================] - 6s 406ms/step - loss: 0.9935 - categorical_accuracy: 0.5958 - val_loss: 0.8800 - val_categorical_accuracy: 0.5625\n",
      "Epoch 3/10\n",
      "15/14 [===============================] - 6s 402ms/step - loss: 1.0004 - categorical_accuracy: 0.5771 - val_loss: 0.8317 - val_categorical_accuracy: 0.5781\n",
      "Epoch 4/10\n",
      "15/14 [===============================] - 6s 408ms/step - loss: 0.9786 - categorical_accuracy: 0.5604 - val_loss: 0.9809 - val_categorical_accuracy: 0.5938\n",
      "Epoch 5/10\n",
      "15/14 [===============================] - 6s 419ms/step - loss: 0.9204 - categorical_accuracy: 0.5833 - val_loss: 0.7491 - val_categorical_accuracy: 0.5859\n",
      "Epoch 6/10\n",
      "15/14 [===============================] - 6s 411ms/step - loss: 0.8426 - categorical_accuracy: 0.6167 - val_loss: 0.9410 - val_categorical_accuracy: 0.6016\n",
      "Epoch 7/10\n",
      "15/14 [===============================] - 6s 404ms/step - loss: 0.8929 - categorical_accuracy: 0.5896 - val_loss: 0.6520 - val_categorical_accuracy: 0.6250\n",
      "Epoch 8/10\n",
      "15/14 [===============================] - 6s 415ms/step - loss: 0.8136 - categorical_accuracy: 0.6313 - val_loss: 0.7039 - val_categorical_accuracy: 0.6172\n",
      "Epoch 9/10\n",
      "15/14 [===============================] - 6s 404ms/step - loss: 0.8082 - categorical_accuracy: 0.6438 - val_loss: 1.1126 - val_categorical_accuracy: 0.6562\n",
      "Epoch 10/10\n",
      "15/14 [===============================] - 6s 409ms/step - loss: 0.8874 - categorical_accuracy: 0.6042 - val_loss: 0.7250 - val_categorical_accuracy: 0.6406\n"
     ]
    }
   ],
   "source": [
    "train_generator(model2, batch_train_iter_aspect, batch_val_iter_aspect, \n",
    "                train_steps_epoch, val_steps_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVPNUQcuyAU3"
   },
   "source": [
    "##Evaluating on test set\n",
    "- show the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "J_JQwuUHMisH",
    "outputId": "3c0441dc-41f0-4013-b928-53ed2070252c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638/638 [==============================] - 1s 2ms/step\n",
      "Categorical accuracy: 58.4639%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model2.evaluate([test_x, test_aspect], test_y)\n",
    "\n",
    "print(\"Categorical accuracy: {:.4f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgZAtHE6xqhH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-3.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
