{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author : Kartik B Bhargav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26717,
     "status": "ok",
     "timestamp": 1589831842033,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "ph91TE5lvthU",
    "outputId": "588cbdf7-3e79-4b92-d961-d3c081cf7b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7BivIvvvzaW"
   },
   "outputs": [],
   "source": [
    "# You will need to change the following directory path according to your own path\n",
    "#cd drive/My Drive/Recsys-2019/sequence_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0VnIJhDRpvc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lM1cs7o-vpAW"
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPslRReavpAZ"
   },
   "outputs": [],
   "source": [
    "aspect_path = '/drive/My Drive/Deep Learing Course/practice-5-data/aspect_level-sentiment/aspect_level/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkm19_52vpAd"
   },
   "outputs": [],
   "source": [
    "doc_path = '/drive/My Drive/Deep Learing Course/practice-5-data/doc_level-sentiment/doc_level/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl50euzvDcQP"
   },
   "outputs": [],
   "source": [
    "### Reading preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Sk_RKQSvpAj"
   },
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwVGqOSavpAm"
   },
   "outputs": [],
   "source": [
    "vocab = read_pickle(aspect_path, 'all_vocab.pkl')\n",
    "\n",
    "train_x = read_pickle(aspect_path, 'train_x.pkl')\n",
    "train_y = read_pickle(aspect_path, 'train_y.pkl')\n",
    "dev_x = read_pickle(aspect_path, 'dev_x.pkl')\n",
    "dev_y = read_pickle(aspect_path, 'dev_y.pkl')\n",
    "test_x = read_pickle(aspect_path, 'test_x.pkl')\n",
    "test_y = read_pickle(aspect_path, 'test_y.pkl')\n",
    "\n",
    "train_aspect = read_pickle(aspect_path, 'train_aspect.pkl')\n",
    "dev_aspect = read_pickle(aspect_path, 'dev_aspect.pkl')\n",
    "test_aspect = read_pickle(aspect_path, 'test_aspect.pkl')\n",
    "\n",
    "\n",
    "pretrain_data = read_pickle(aspect_path, 'pretrain_data.pkl')\n",
    "pretrain_label = read_pickle(aspect_path, 'pretrain_label.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBmfsDKdvpAu"
   },
   "source": [
    "### Batch generator and data iterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_b-5_EPvpAv"
   },
   "outputs": [],
   "source": [
    "class Dataiterator():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, aspect_data, doc_data, seq_length=32, decoder_dim=300, batch_size=32):\n",
    "        \n",
    "        len_aspect_data = len(aspect_data[0])\n",
    "        self.len_doc_data = len(doc_data[0])\n",
    "        \n",
    "        self.X_aspect = aspect_data[0] \n",
    "        self.y_aspect = aspect_data[1]\n",
    "        self.aspect_terms = aspect_data[2]\n",
    "        \n",
    "        self.X_doc = doc_data[0]\n",
    "        self.y_doc = doc_data[1]\n",
    "        \n",
    "        self.num_data = len_aspect_data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        \n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X_aspect = self.X_aspect[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y_aspect = self.y_aspect[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        batch_aspect_terms = self.aspect_terms[np.array(X_ids)]\n",
    "        indices_2 = np.random.choice(self.len_doc_data, self.batch_size)\n",
    "        batch_X_doc = self.X_doc[indices_2]\n",
    "        batch_y_doc = self.y_doc[indices_2]\n",
    "        \n",
    "        \n",
    "        return batch_X_aspect, batch_y_aspect, batch_aspect_terms, batch_X_doc, batch_y_doc\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X_aspect, self.y_aspect, self.aspect_terms, self.X_doc, self.y_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sklA6jSUWDS6"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDU0hXVJ1PR5"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Lambda, Dropout, LSTM\n",
    "from keras.layers import Reshape, Activation, RepeatVector, concatenate, Concatenate, Dot, Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pvg-Uf2h_6Qm"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dnbn0_O0XlGf"
   },
   "source": [
    "### Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yraOrwX3vpA8"
   },
   "outputs": [],
   "source": [
    "overal_maxlen = 82\n",
    "overal_maxlen_aspect = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjiAsayW1zx9"
   },
   "outputs": [],
   "source": [
    "class Custom_softmax(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Custom_softmax, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        if self.mask_zero:\n",
    "            a = K.exp(x)         \n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            a = a * mask\n",
    "            a=a / (K.sum(a, axis=1, keepdims=True) + K.epsilon())\n",
    "            return a\n",
    "        else:\n",
    "            return K.softmax(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1],1)\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp5xh27GXmva"
   },
   "outputs": [],
   "source": [
    "repeator = RepeatVector(overal_maxlen, name='repeator_att')\n",
    "concatenator = Concatenate(axis=-1, name='concator_att')\n",
    "densor1 = Dense(300, activation = \"tanh\", name='densor1_att')\n",
    "densor2 = Dense(1, activation = \"relu\", name='densor2_att')\n",
    "activator = Custom_softmax(mask_zero=True,name='softmax_att')\n",
    "dotor = Dot(axes = 1, name='dotor_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VH3g4z6R8O3_"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################################\n",
    "#### Shape of Keys:[batch-size, overal-maxlen,Dimension_output_lstm], Shape of query: [batch-size,1,Dimension-output-wordEmbedding]  ###\n",
    "########################################################################################################################################\n",
    "def attention(keys, query):\n",
    "    \n",
    "    query = repeator(query)\n",
    "    print(\"query shape: %s\" %str(query._keras_shape))\n",
    "    concat = concatenator([keys, query])\n",
    "    print(\"concat shape: %s\" %str(concat._keras_shape))\n",
    "    e1 = densor1(concat)\n",
    "    print(\"e1 shape: %s\" %str(e1._keras_shape))\n",
    "    e2 = densor2(e1)\n",
    "    print(\"e2 shape: %s\" %str(e2._keras_shape))\n",
    "    alphas = activator(e2)\n",
    "    print(\"alphas shape: %s\" %str(alphas._keras_shape))\n",
    "    context = dotor([alphas, keys])\n",
    "    print(\"context shape: %s\" %str(context._keras_shape))\n",
    "    \n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVmvIn0X_0Ji"
   },
   "outputs": [],
   "source": [
    "class Average(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Average, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        if self.mask_zero:           \n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=1) / (K.sum(mask, axis=1) + K.epsilon())\n",
    "        else:\n",
    "            return K.mean(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsyFHokczuvO"
   },
   "source": [
    "### Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBdo6q2QvpBW"
   },
   "outputs": [],
   "source": [
    "dropout = 0.5     \n",
    "recurrent_dropout = 0.1\n",
    "vocab_size = len(vocab)\n",
    "num_outputs = 3 # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQVV-MRRvpBc"
   },
   "source": [
    "### Inputs: How many inputs do you need for the current task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJ7tv-7GvpBd"
   },
   "outputs": [],
   "source": [
    "##### Inputs #####\n",
    "sentence_input = Input(shape=(overal_maxlen,), dtype='int32', name='sentence_input')\n",
    "aspect_input = Input(shape=(overal_maxlen_aspect,), dtype='int32', name='aspect_input')\n",
    "pretrain_input = Input(shape=(None,), dtype='int32', name='pretrain_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58PLwihDvpBk"
   },
   "source": [
    "### Word-level embedding (shareable between all model inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJxzja4GvpBm"
   },
   "outputs": [],
   "source": [
    "##### construct word embedding layer #####\n",
    "word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaWk1GuHvpB1"
   },
   "source": [
    "### Aspect-level representation (averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1589836858720,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "Q9q7v6TyvpB2",
    "outputId": "bb35d09a-92b8-463e-ac98-f75f39092a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use average term embs as aspect embedding\n"
     ]
    }
   ],
   "source": [
    "### represent aspect as averaged word embedding ###\n",
    "print ('use average term embs as aspect embedding')\n",
    "aspect_term_embs = word_emb(aspect_input)\n",
    "aspect_embs = Average(mask_zero=True, name='aspect_emb')(aspect_term_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZIcsq-SvpCF"
   },
   "source": [
    "### Sentence-level representation from two domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRCN6JX4vpCH"
   },
   "outputs": [],
   "source": [
    "### sentence representation ###\n",
    "sentence_embs = word_emb(sentence_input) # from aspect-level domain\n",
    "pretrain_embs = word_emb(pretrain_input) # from document-level domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MzAALsZMrxM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wvA7CahvpCL"
   },
   "source": [
    "### LSTM layer (shared between three representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEJg_StjvpCN"
   },
   "outputs": [],
   "source": [
    "rnn = LSTM(300, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, name='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1589836863113,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "2JpWf9bQvpCR",
    "outputId": "7d9bb6be-f363-4c45-8437-ef02daa452b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 82, 300)\n"
     ]
    }
   ],
   "source": [
    "### sentence representation ###\n",
    "sentence_lstm = rnn(sentence_embs)    # from aspect-level domain\n",
    "pretrain_lstm = rnn(pretrain_embs)     # from document-level domain\n",
    "print(sentence_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtrvodzrvpCV"
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT REPLACE KEYS?, QUERY? WITH THE CORRESPONDING TENSORS AS ATTENTION KEYS AND QUERY\n",
    "\n",
    "#att_context, att_weights = attention(KEYS?, QUERY?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1589836863930,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "LnYbFaqPvpCX",
    "outputId": "0c8343cc-01dd-4aa8-9a1c-c8097eb22e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (None, 82, 300)\n",
      "concat shape: (None, 82, 600)\n",
      "e1 shape: (None, 82, 300)\n",
      "e2 shape: (None, 82, 1)\n",
      "alphas shape: (None, 82, 1)\n",
      "context shape: (None, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "att_context,att_weights=attention(sentence_lstm,aspect_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1589836865359,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "wmAFYpc5vpCa",
    "outputId": "6da36aef-f6cd-4e06-c221-8166bb62b5d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "(None, 300)\n"
     ]
    }
   ],
   "source": [
    "pretrain_avg = Average(mask_zero=True)(pretrain_lstm)\n",
    "print(att_context.shape)\n",
    "print(pretrain_avg.shape)\n",
    "sentence_output = Dense(num_outputs, name='dense_1')(att_context)\n",
    "pretrain_output = Dense(num_outputs, name='dense_2')(pretrain_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1589836865361,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "rAlpreOvvpCd",
    "outputId": "77095032-5624-44c4-f98e-b452f2c36217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_output shape: (None, 1, 3)\n",
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"sentence_output shape: %s\" % str(sentence_output._keras_shape))\n",
    "print(pretrain_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T74EglluvpCh"
   },
   "outputs": [],
   "source": [
    "sentence_output = Reshape((num_outputs,))(sentence_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1589836867823,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "yAFar6sZvpCl",
    "outputId": "b7aeaf64-6113-4581-f6e5-e1570c074ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_output shape: (None, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"sentence_output shape: %s\" % str(sentence_output._keras_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_EmohJBvpCr"
   },
   "outputs": [],
   "source": [
    "aspect_probs = Activation('softmax', name='aspect_model')(sentence_output)\n",
    "doc_probs = Activation('softmax', name='pretrain_model')(pretrain_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6ptr7iuWIYT"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[sentence_input, aspect_input, pretrain_input], outputs=[aspect_probs, doc_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmPSF8IAENj_"
   },
   "outputs": [],
   "source": [
    "import keras.optimizers as opt\n",
    "\n",
    "optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1589836868034,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "eiFkEy-eVtoU",
    "outputId": "93221eea-e36d-47bf-9b6e-583fe601223d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_input (InputLayer)     (None, 82)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "aspect_input (InputLayer)       (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_emb (Embedding)            multiple             3000900     aspect_input[0][0]               \n",
      "                                                                 sentence_input[0][0]             \n",
      "                                                                 pretrain_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "aspect_emb (Average)            (None, 300)          0           word_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     multiple             721200      word_emb[1][0]                   \n",
      "                                                                 word_emb[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeator_att (RepeatVector)     (None, 82, 300)      0           aspect_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concator_att (Concatenate)      (None, 82, 600)      0           lstm[0][0]                       \n",
      "                                                                 repeator_att[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "densor1_att (Dense)             (None, 82, 300)      180300      concator_att[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "densor2_att (Dense)             (None, 82, 1)        301         densor1_att[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pretrain_input (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "softmax_att (Custom_softmax)    (None, 82, 1)        0           densor2_att[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dotor_att (Dot)                 (None, 1, 300)       0           softmax_att[0][0]                \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 3)         903         dotor_att[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_3 (Average)             (None, 300)          0           lstm[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 3)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            903         average_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "aspect_model (Activation)       (None, 3)            0           reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pretrain_model (Activation)     (None, 3)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,904,507\n",
      "Trainable params: 3,904,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaNcPdHiC650"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss={'aspect_model': 'categorical_crossentropy', 'pretrain_model': 'categorical_crossentropy'},\n",
    "              loss_weights = {'aspect_model': 1, 'pretrain_model': 0.1},\n",
    "              metrics = {'aspect_model': 'categorical_accuracy', 'pretrain_model': 'categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilIKUGn_DEgB"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi0coTFsDWFG"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1589836872289,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "xETYnVpgvpC5",
    "outputId": "1a1d4276-83bf-4426-80ec-432a1ddf8d3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 206,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HviIfa5lvpC7"
   },
   "outputs": [],
   "source": [
    "pretrain_data= np.array(pretrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Y3_eO4QgyC3"
   },
   "outputs": [],
   "source": [
    "train_steps_epoch = len(train_x)/batch_size\n",
    "batch_train_iter = Dataiterator([train_x, train_y, train_aspect], \\\n",
    "                                [pretrain_data, pretrain_label], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NvFUoJlvpDA"
   },
   "outputs": [],
   "source": [
    "val_steps_epoch = len(dev_x)/batch_size\n",
    "batch_val_iter = Dataiterator([dev_x, dev_y, dev_aspect], \\\n",
    "                              [pretrain_data, pretrain_label], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVb-obDevpDD"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_generator(model, batch_train_iter, batch_val_iter):\n",
    "    \n",
    "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
    "                                     monitor='val_loss', save_best_only=False, \\\n",
    "                                     save_weights_only=True)\n",
    "                     ]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[[X, aspect, pretrain_X], [y, pretrain_y]] for X, y, \\\n",
    "                             aspect, pretrain_X, pretrain_y in batch_train_iter]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "                \n",
    "    def val_gen():\n",
    "        while True:\n",
    "            val_batches = [[[X, aspect, pretrain_X], [y, pretrain_y]] for X, y, \\\n",
    "                           aspect, pretrain_X, pretrain_y in batch_val_iter]\n",
    "            for val_batch in val_batches:\n",
    "                yield val_batch\n",
    "                \n",
    "    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n",
    "                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n",
    "                                  epochs = 10, callbacks = earlystop_callbacks)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 897387,
     "status": "ok",
     "timestamp": 1589837772786,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "wFyKY0ngvpDI",
    "outputId": "3fffd185-541b-4255-882f-e2eae034b07b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "58/57 [==============================] - 92s 2s/step - loss: 1.0709 - aspect_model_loss: 0.9614 - pretrain_model_loss: 1.0949 - aspect_model_categorical_accuracy: 0.5673 - pretrain_model_categorical_accuracy: 0.3739 - val_loss: 0.9534 - val_aspect_model_loss: 0.8635 - val_pretrain_model_loss: 1.0677 - val_aspect_model_categorical_accuracy: 0.6313 - val_pretrain_model_categorical_accuracy: 0.4354\n",
      "Epoch 2/10\n",
      "58/57 [==============================] - 90s 2s/step - loss: 0.8093 - aspect_model_loss: 0.7064 - pretrain_model_loss: 1.0290 - aspect_model_categorical_accuracy: 0.7209 - pretrain_model_categorical_accuracy: 0.4844 - val_loss: 0.9283 - val_aspect_model_loss: 0.9174 - val_pretrain_model_loss: 0.9874 - val_aspect_model_categorical_accuracy: 0.5729 - val_pretrain_model_categorical_accuracy: 0.4750\n",
      "Epoch 3/10\n",
      "58/57 [==============================] - 89s 2s/step - loss: 0.6820 - aspect_model_loss: 0.5830 - pretrain_model_loss: 0.9892 - aspect_model_categorical_accuracy: 0.7796 - pretrain_model_categorical_accuracy: 0.5135 - val_loss: 0.9613 - val_aspect_model_loss: 0.9246 - val_pretrain_model_loss: 0.9434 - val_aspect_model_categorical_accuracy: 0.6417 - val_pretrain_model_categorical_accuracy: 0.5250\n",
      "Epoch 4/10\n",
      "58/57 [==============================] - 88s 2s/step - loss: 0.6227 - aspect_model_loss: 0.5264 - pretrain_model_loss: 0.9630 - aspect_model_categorical_accuracy: 0.8071 - pretrain_model_categorical_accuracy: 0.5129 - val_loss: 0.7555 - val_aspect_model_loss: 0.8967 - val_pretrain_model_loss: 0.9257 - val_aspect_model_categorical_accuracy: 0.6125 - val_pretrain_model_categorical_accuracy: 0.5312\n",
      "Epoch 5/10\n",
      "58/57 [==============================] - 88s 2s/step - loss: 0.5431 - aspect_model_loss: 0.4508 - pretrain_model_loss: 0.9226 - aspect_model_categorical_accuracy: 0.8384 - pretrain_model_categorical_accuracy: 0.5447 - val_loss: 1.4550 - val_aspect_model_loss: 1.0070 - val_pretrain_model_loss: 0.9708 - val_aspect_model_categorical_accuracy: 0.6583 - val_pretrain_model_categorical_accuracy: 0.5146\n",
      "Epoch 6/10\n",
      "58/57 [==============================] - 88s 2s/step - loss: 0.5238 - aspect_model_loss: 0.4312 - pretrain_model_loss: 0.9266 - aspect_model_categorical_accuracy: 0.8411 - pretrain_model_categorical_accuracy: 0.5356 - val_loss: 0.9513 - val_aspect_model_loss: 0.9396 - val_pretrain_model_loss: 0.8881 - val_aspect_model_categorical_accuracy: 0.6271 - val_pretrain_model_categorical_accuracy: 0.5583\n",
      "Epoch 7/10\n",
      "58/57 [==============================] - 89s 2s/step - loss: 0.4811 - aspect_model_loss: 0.3898 - pretrain_model_loss: 0.9127 - aspect_model_categorical_accuracy: 0.8534 - pretrain_model_categorical_accuracy: 0.5426 - val_loss: 1.2744 - val_aspect_model_loss: 0.9598 - val_pretrain_model_loss: 0.8824 - val_aspect_model_categorical_accuracy: 0.6292 - val_pretrain_model_categorical_accuracy: 0.5750\n",
      "Epoch 8/10\n",
      "58/57 [==============================] - 90s 2s/step - loss: 0.4638 - aspect_model_loss: 0.3737 - pretrain_model_loss: 0.9013 - aspect_model_categorical_accuracy: 0.8702 - pretrain_model_categorical_accuracy: 0.5539 - val_loss: 1.0791 - val_aspect_model_loss: 0.8765 - val_pretrain_model_loss: 0.8539 - val_aspect_model_categorical_accuracy: 0.6896 - val_pretrain_model_categorical_accuracy: 0.5771\n",
      "Epoch 9/10\n",
      "58/57 [==============================] - 90s 2s/step - loss: 0.4155 - aspect_model_loss: 0.3272 - pretrain_model_loss: 0.8825 - aspect_model_categorical_accuracy: 0.8836 - pretrain_model_categorical_accuracy: 0.5814 - val_loss: 1.3944 - val_aspect_model_loss: 1.0983 - val_pretrain_model_loss: 0.9142 - val_aspect_model_categorical_accuracy: 0.6396 - val_pretrain_model_categorical_accuracy: 0.5583\n",
      "Epoch 10/10\n",
      "58/57 [==============================] - 89s 2s/step - loss: 0.3952 - aspect_model_loss: 0.3075 - pretrain_model_loss: 0.8767 - aspect_model_categorical_accuracy: 0.8847 - pretrain_model_categorical_accuracy: 0.5593 - val_loss: 1.0744 - val_aspect_model_loss: 1.0457 - val_pretrain_model_loss: 0.8634 - val_aspect_model_categorical_accuracy: 0.6458 - val_pretrain_model_categorical_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "train_generator(model, batch_train_iter, batch_val_iter)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uaWk1GuHvpB1",
    "-ZIcsq-SvpCF"
   ],
   "name": "Practical_5_1_2_Aspect_Level_Sentiment_ANSWER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
